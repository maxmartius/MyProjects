[{"lecture_name": "2_ML_Validity.pdf", "is_lecture": true, "paragraph": "The text discusses the topic of software engineering for AI-enabled systems. It mentions various components and processes involved in developing such systems, including feedback mechanisms, archetypes, life cycle, metrics, data management, model selection and training. The text also raises questions about how to define experiments, ensure validity of results, derive meaningful metrics and make experiments reproducible in order to improve existing solutions."}, {"lecture_name": "2_ML_Validity.pdf", "is_lecture": true, "paragraph": "The text discusses the validity of AI experiments and provides some key points to consider. It emphasizes the importance of aligning experiments with the application goal, avoiding information leakage from test to training, making setups reproducible and explicit for transparency and easy debugging, and understanding important evaluation metrics. The text also mentions examples of AI hype, such as Facebook's struggle with moderation at a global scale due to cultural nuances and IBM Watson's canceled collaboration in eradicating cancer. It warns about the risks associated with autonomous systems that can cause physical harm if accessed by bad actors or affected by glitches and errors. Additionally, it highlights issues like misinterpretation, blindly using machine learning on stochastic problems, finding spurious correlations in large datasets leading to false results (reproducibility crisis), and p-hacking."}, {"lecture_name": "2_ML_Validity.pdf", "is_lecture": true, "paragraph": "The text discusses various issues that can threaten the validity of experiments in data science. These include competing risk, informative censoring, publication bias, spin, immortal time bias, selection bias, Simpson's paradox, and more. The text then describes a scenario for a running example where the research goal is to estimate execution time of functions without executing the code. It outlines steps such as feature selection and algorithm selection before starting experimentation. The importance of looking at the data and formulating expectations is emphasized. The text also mentions outlier removal and value range considerations in preprocessing the data. Experimental setup is discussed with a focus on hypothesis formulation and controlling validity threats like internal validity (selection bias) and external validity (generalization error). Finally, it explains independent variables and dependent variables in data science experiments along with forms of validity including internal validity (data leakage) and external validity (sampling bias)."}, {"lecture_name": "2_ML_Validity.pdf", "is_lecture": true, "paragraph": "The text discusses metrics and baselines in experiment analysis. Metrics are indicators used to measure quantitative properties of interest, such as accuracy or edit distance. Baselines refer to the achieved value/score of a metric for a known process and act as minimal reachable targets for new processes/models.\n\nIn speech recognition, metrics can be used to rate the quality of the model and link to non-functional requirements like inference time. It is often not feasible or necessary to improve all metrics simultaneously, so it is recommended to focus on 1-2 metrics while setting thresholds for others using baselines.\n\nEstablishing baselines can be done through methods like asking human subjects, conducting literature searches, implementing quick-and-dirty solutions, or using performance data from prior systems.\n\nDifferent measures are available for classification tasks including miss-rate (measuring how many instances are missed), recall (fraction of correctly classified instances), precision (how often correct predictions are made), false positive rate (frequency of misclassifications), harmonic mean/F1-score (combining precision and recall), and ROC curves which plot true positive rate against false positive rate.\n\nInternal validity involves measuring bias in performance patterns while repetitions help assess variations in measurements. External validity focuses on generalization by evaluating models' ability to generalize beyond training data.\n\nGeneralization error can be assessed with test sets that were not used during training. Bias-variance tradeoff refers to finding an optimal balance between complexity assumptions (bias) and sensitivity towards changes in training data (variance).\n\nK-fold cross-validation allows multiple runs with different validation splits for better estimation of generalization error.\n\nLastly, autocorrelation is identified as a problem when predicting values over time steps but can be addressed by predicting differences instead."}]